\chapter{Evaluierung}
\label{chp:Evaluation}
  \citet{distrh2} haben gezeigt, dass deren paralleler Ansatz sehr nah an die optimale parallele Effizienz von $\mathcal{O}(\frac{nk}{p})$ herankommt, falls $n$ deutlich größer als $p$ ist.
  Dabei gilt: $n := |\Omega|$ ist die Anzahl Sonnen, $p := |\Proc|$ ist die Anzahl Prozesse und $k := k_1^3$ ist die Anzahl Interpolationspunkte, mit dem Grad der eindimensionalen Lagrange-Polynomen 
  $k_1$. Außerdem sei $m := \frac{n}{p}$ die durchschnittliche Anzahl Elemente pro Prozess.
  
  Im folgenden gilt es diese Argumentation auf den vorliegenden Algorithmus zu übertragen und durch Daten aus praktischen Laufzeitmessungen zu unterstützen. Da unser Algorithmus mit impliziten
  Blockbäumen und \hquad arbeitet, wird die Konstruktion nicht weiter behandelt. Statt dessen konzentrieren wir uns auf eine Abschätzung für die Laufzeit.
  
  \section{Theoretische Abschätzung}
  
  Zunächst treffen wir auch für diesen Abschnitt eine Annahme, die bereits bei der Vorstellung des Algorithmus zielführend war:
  \begin{ann} \ \\
  \label{ann:nodes}
    Es existiert $q \in \N$ sodass für $p := \left| \Proc \right|$ gilt:
    \[ p = 2^q \]
  \end{ann}
  
  Außerdem wollen wir einige Eigenschaften des Clusterbaumes annehmen:
  
  \begin{ann} \ \\
  \label{ann:tree}
    Es existiert eine Konstante $C_{st}$, sodass für alle Teilbäume $T_{sub}$ mit $\tau := root(T_{sub}) \in T_\Omega^{(q)}$ gilt
    \[
     |T_{sub}|\footnotemark \leq C_{st} \frac{n}{kp} \text{ und}
    \]
    \[
     |\tau| \leq C_{st}\frac{n}{p}
    \]
  \end{ann}
  \footnotetext{$|T_{sub}|$ bezeichnet die Anzahl Knoten im Baum}
  
  Diese Annahme wurde durch die Wahl der Abbruchbedingung bei der Konstruktion des Clusterbaumes sichergestellt. Es gilt $|T_{sub}| = \frac{1}{2} \frac{n}{kp}$ (vgl. \autoref{eq:l}),
  da jeder Prozess gerade einen Teilbaum konstruiert, wie er zuvor vom gesamten nicht-parallelisierten Algorithmus vorgenommen wurde (vgl. letzter Absatz in \autoref{sec:data}). Zwar lässt sich die
  Anzahl Elemente der Cluster $\tau \in T_\Omega^{(q)}$ nicht exakt angeben, da die Unterteilung in Sohncluster nicht nach Kardinalität vorgenommen wird, jedoch beträgt diese im Schnitt gerade 
  $\frac{n}{p}$. Die Konstante $C_{st}$ kann also als $\approx 1$ angenommen werden.
  
  Die Methode \code{_setup(Cluster *c, int depth)}, die in unserem Algorithmus die Verteilung der Cluster auf Prozesse vornimmt, gewährleistet folgende Eigenschaften:
  
  \begin{bem}
    (Zuständigkeiten)\\
    Für den verteilten Clusterbaum $T_\Omega$ gelten folgende Eigenschaften:
    \begin{equation}
      \text{Für } \tau \in T_\Omega^{(\geq q)} \text{ existiert genau ein } P \in \Proc \text{ mit } \tau.\tcode{activ} = id_P.
    \end{equation}
    \begin{equation}
      \text{Jeder Prozess } P \in \Proc \text{ berechnet auf jeder Ebene } T_\Omega^{(q')}, \ q' \in \nullhaken{q-1} \text{ genau eine Transfermatrix.}\label{eq:log}
    \end{equation}

    Aus \autoref{eq:log} folgt direkt mit $q = \log_2 p$ (vgl. \autoref{sec:work}):
    \begin{equation}
      \text{Die Anzahl Transfermatrizen } E_\tau, \ \tau \in T_\Omega^{(\leq q)} \text{ pro Prozess beträgt } q.\tag{\ref{eq:log}'}
    \end{equation}
  \end{bem}

  Da wir einen parallel arbeitenden Algorithmus haben ist es für die Abschätzung des Rechenaufwandes nicht ausreichend die Anzahl an Operationen zu zählen. Da die Prozesse kommunizieren müssen ist 
  regelmäßig eine Synchronisation der Prozesse notwendig. So wird vorkommen, dass ein Prozess $P_i$ schneller seine Berechnungen durchführt als ein Prozess $P_j$ und dann auf diesen warten muss bevor
  die Kommunikation stattfinden kann. 
  
  Daher verwenden wir einen ähnlichen Ansatz, wie er auch beim BSP-Modell \citep{bsp} verwendet wurde: Die gesamte Berechnung wird in eine Sequenz von $s \in \N_0$ \textit{Superschritten} eingeteilt,
  die jeweils unabhängig von den anderen Prozessen von einem Prozess durchgeführt werden können. Der $i$-te Superschritt startet simultan, sobald alle Prozesse den $i-1$-ten Superschritt abgeschlossen
  haben, $i \in \haken{s}$. Als Zeiteinheit verwenden wir die abstrahierte Einheit \textit{Zyklus}. Ein Zyklus sei dabei lang genug um eine arithmetische Operation, einen Speicherzugriff oder eine Sende-
  oder Empfangsoperation für einen \code{double}-Wert durchzuführen.
  
  \begin{lem}
  \label{lem:vorw}
    (Vorwärtstransformation)\\
    Die parallele Vorwärtstransformation benötigt $\mathcal{O}(\frac{nk}{p}+k^2log_2p)$ Zyklen.
  \end{lem}
  
  \textit{Beweis:} 
  Die gesamte Vorwärtstransformation kann in einem Superschritt abgehandelt werden, da keinerlei Kommunikation notwendig ist. 
  
  Zunächst werden alle aktiven Cluster ausgewertet. Seien also $P \in \Proc$ und $T_\Omega^P$ der aktive Teilbaum dieses Prozesses. Für ein Blattcluster $\sigma \in \mathcal{L}(T_\Omega^P)$ müssen 
  für alle $k$ Ersatzmassen $|\sigma|$ Berechnungen durchgeführt werden.  
  Dann gilt  mit \autoref{ann:tree}, dass insgesamt
  \begin{align*}
    \sum_{\sigma \in \mathcal{L}(T_\Omega^P)} k|\sigma| &= k\sum_{\sigma \in \mathcal{L}(T_\Omega^P)} |\sigma| \approx k m\\
    &\leq C_{st}\frac{nk}{p}
  \end{align*}
  Operationen ausgeführt werden müssen.
  
  Für Nicht-Blattcluster $\sigma \in T_\Omega^P \backslash mathcal{L}(T_\Omega^P)$ sind für die $k$ Ersatzmassen und beide Sohncluster wiederum $k$ Auswertungen notwendig. Damit folgt, dass weitere
  \begin{align*}
    &\sum_{\sigma \in T_\Omega^P \backslash \mathcal{L}(T_\Omega^P)} \ \ \sum_{\tilde \sigma \in sons(\sigma)} 2k^2 \leq \sum_{\tilde \sigma \in T_\Omega^P} 2k^2\\
    &= 2k^2 |T_\Omega^P| \leq 2k^2 C_{st} \frac{n}{kp} = 2C_{st}\frac{nk}{p}
  \end{align*}
  Operationen ausgeführt werden müssen.
  
  Für die Ebenen $T_\Omega^{(<q)}$ werden nun jeweils die $k$ Ersatzmassen aus den $k$ Ersatzmassen eines Sohnclusters berechnet. Da es gerade $q = log_2(p)$ Ebenen gibt führt dies insgesamt zu obiger
  Abschätzung. \qed
  
  \begin{lem}
  \label{lem:ruckw}
    (Rückwärtstransformation)\\
    Die parallele Rückwärtstransformation benötigt $\mathcal{O}(\frac{nk}{p}+k^2log_2p)$ Zyklen.
  \end{lem}
  
  \textit{Beweis:}
  Der Beweis läuft analog zu dem von \autoref{lem:vorw}. Der Algorithmus ist im Prinzip der selbe, die Berechnung verläuft lediglich von der Wurzel zu den Blättern statt wie bei der 
  Vorwärtstransformation umgekehrt. \qed
  
  \begin{lem}
  \label{lem:koppl}
    (Kopplungsmatrizen und Nahfeld)
    Die Auswertung der Kopplungs- und Nahfeldmatrizen benötigt $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$ Zyklen.
  \end{lem}

  \textit{Beweis:}
  Von \citet{distrh2} wurde bewiesen, dass der Speicherbedarf für Nah- und Fernfeldmatrizen $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$ beträgt. Da der Algorithmus für jedes Element der Matrizen $S_b$ und
  $N_b$ nicht mehr als zwei Auswertungen vornimmt, liegt auch die benötigte Anzahl Zyklen in $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$. Auch in diesem Schritt ist keine Kommunikation notwendig, sodass 
  auch diese Auswertung in einem Superschritt stattfinden kann. \qed
  
  Bleibt noch die Kommunikation. Diese gliedert sich in zwei Superschritte: Das Vorbereiten mit dem anschließenden Senden der Daten und das Empfangen der Daten. Da die Vorbereitung der Kommunikation 
  nach der selben Struktur wie die Kopplungs- und Nahfeldmatrizen arbeitet, fallen dafür ebenfalls $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$ Zyklen an.
  Da ebenso viele Elemente gesendet und empfangen werden müssen liegt der Gesamtaufwand für die Kommunikation in eben dieser Komplexitätsklasse. Für jeden Superschritt wurde also gezeigt, dass dieser
  $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$ Zyklen benötigt. Damit folgt für den gesamten Algorithmus:
  
  \begin{thm}
    Die Berechnung des Algorithmus benötigt $\mathcal{O}(\frac{nk}{p} + k^2 log_2p)$ Zyklen.
  \end{thm}
  
  \citet{distrh2} merken noch an, dass der Faktor $k^2 log_2p$ für genügend große Probleme verschwindend geringen Einfluss haben wird.
  
  \section{Laufzeitmessung}
  Die theoretischen Argumente des letzten Abschnitts wollen wir nun durch die Messung von Laufzeitdaten des Algorithmus untermauern.
  
  Um diese Daten zu sammeln, wurde das vorliegende auf dem NEC HPC-Linux-Cluster der CAU Kiel ausgeführt. Jeder Knoten dieses Clusters ist mir 192 GB Arbeitsspeicher und zwei Intel Xeon Gold 
  6130 bestückt, die einen Kerntakt von 2,1 GHz aufweisen. Verbunden sind die Knoten über EDR infiniband. Als Compiler wurden der Intel-C-Compiler 17.0.4 sowie der Intel-MPI-Compiler 17.0.4 verwendet.
  
  Es wurden unterschiedliche Testreihen durchgeführt, bei denen entweder die Anzahl Prozesse oder die Anzahl Sonnen variiert wurde, um beide Ansätze unabhängig voneinander testen zu können.
  Alle Tests wurden mit Lagrange-Polynomen von Grad $3$ in jeder Richtung approximiert, was zu $3^3 = 27$ Interpolationspunkten führt. Außerdem wurde jeder Testlauf 10 Mal wiederholt und immer 
  vollständige Knoten im Cluster angefordert, um stabile Daten zu ermitteln.
  
  \subsubsection{Approximation}
  \input{chapters/1-32x_fig}
  \input{chapters/1-32x_tab}
  \begin{table}[b]
    \begin{tabular}{|l|c c c c c c c|}
    \hline
    $k_0$ & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
    \hline
    $k$ & 1 & 8 & 27 & 64 & 125 & 216 & 343\\
    \hline
    Fehler & $5,12e^{-2}$ & $8,58e^{-3}$ & $8,11e^{-4}$ & $9,38e^{-5}$ & $9,67e^{-6}$ & $1,21e^{-6}$ & $7,4e^{-15}$\\
    \hline
    Laufzeit [s] & 0,0486 & 0,0967 & 0,259 & 0,454 & 0,864 & 1,09 & 1,3\\
    \hline
    \end{tabular}
    \caption{Approximationsfehler und Laufzeit in Abhängigkeit zur gewählten Interpolationsordnung.}
    \label{tab:error}
  \end{table}
  
  In den ersten beiden Testreihen wurde die Anzahl Sonnen pro Prozess bei konstanter Anzahl Prozesse variiert. Die Messergebnisse sind in \autoref{tab:1-32x} sowie in \autoref{fig:1-32x} 
  aufgeführt.
  
  Abbildung beziehungsweise Tabelle \textbf{(a)} bezieht sich jeweils auf die Testreihe mit einem Prozess, \textbf{(b)} auf die Testreihe mit 32 Prozessen. Trotz einiger Schwankungen, deren Herkunft
  nicht festgestellt werden konnte, ist der lineare Zuwachs gut zu erkennen. Bei der Geraden handelt es sich um eine von gnuplot berechnete Regressionsgrade. Der Testlauf mit $32$ Prozessen zeigt
  eine etwas stärkere Steigung, die vermutlich auf den erhöhten Kommunikationsaufwand zurückzuführen ist. Dennoch ist erkennen, dass die Approximation durch Interpolation und die Nutzung der
  $\mathcal{H}^2$-Struktur den theoretischen Berechnungen auch in reellen Anwendungen gerecht wird und die Komplexität von $\mathcal{O}(n^2)$ auf $\mathcal{O}(k n)$ senken kann. Da $k$ wesentlich 
  kleiner als $n$ ist, ist damit viel gewonnen.
  
  Wir haben nun, ohne weiter Gedanken an die Auswirkungen zu verschwenden, eine Approximation durchgeführt. Zwar haben wir über eine Zulässigkeitsbedingung sichergestellt, dass die Ergebnisse
  ``vernünftig'' sind, aber wie ungenau wird die Berechnung durch die Approximation? Dazu wurden im kleineren Maßstab\footnote{} einige Testläufe durchgeführt, die ebenfalls sehr stabile Resultate
  gezeigt haben. Die Ergebnisse sind in \autoref{tab:error} zu finden.
  
  Wie an den Daten zu erkennen ist, nimmt der Fehler bei jeder Erhöhung der Interpolationsordnung $k_0$ um etwa Faktor $8$ ab. Wir erreichen also schnell eine sehr gute Approximation. Natürlich
  dauert die Berechnung für mehr Interpolationspunkte länger. Zu erkennen ist außerdem, dass bei diesem Testlauf bei $k_0=7$ eine Art Plateau erreicht wurde. Weitere Erhöhung hat keine Signifikante
  Änderung der Fehlerwerte mehr ergeben. Die scheint mit den Ebenen des Clusterbaumes zusammenzuhängen. Wird die Interpolationsordnung gegenüber der Anzahl Sonnen groß genug, werden durch den
  vorliegenden Algorithmus zu wenig Ebenen im Clusterbaum erzeugt, sodass entsprechend wenig zulässige Blöcke gefunden werden können. Dies erklärt sowohl den plötzlichen Anstieg an Genauigkeit und
  die Anschließende Stagnation.
  
  \subsubsection{Parallelität}
  \input{chapters/x20}
  Bereits durch die höhere Steigung der Graden in \autoref{fig:1-32x}.\textbf{(b)} gegenüber der in \textbf{(a)} lässt sich erahnen, dass durch die Kommunikation bei der Parallelisierung ein gewisser
  Overhead verursacht wird. Um dies genauer zu untersuchen wurde die Anzahl Sonnen pro Prozess\footnote{Mit einer Verdoppelung der Prozessanzahl geht also auch eine Verdopplung der Anzahl Sonnen 
  einher.} auf $m = 2^{20}$ fixiert und die Anzahl Prozesse variiert.  Die Messdaten sind in \autoref{tab:x20} und \autoref{fig:x20} zu finden. 
  
  Die Daten unterliegen leider recht großen Schwankungen. Woher diese Schwankungen stammen muss an anderer Stelle genauer untersucht werden, da es den Umfang und die Zielsetzung dieser Arbeit 
  übersteigt. Allerdings waren diese Ergebnisse in sich sehr stabil. Auch mehrfache Wiederholungen der einzelnen Testläufe haben immer bis auf wenige Zehntelsekunden die selbe Zeit benötigt. 
   Trotzdem scheint mir, dass die errechnete Regressionsgrade die allgemeine Tendenz der Laufzeit einigermaßen akkurat wiedergibt.
  
  Ein gewisser Anstieg der Laufzeit ist zu erwarten, da der Kommunikationsaufwand steigt. Insgesamt zeigt sich aber die fast optimale Ausnutzung der Parallelität.
  
  Ein Interessanter Effekt kann an den mit Fußnoten \textit{a} und \textit{b} gekennzeichneten Testläufen beobachtet werden. Während bei \textit{a} alle Prozesse auf einem Knoten gearbeitet haben, 
  wurden diese für den Testlauf \textit{b} auf zwei Knoten verteilt. Eine Vermutung, die man hätte anstellen können, wäre, dass die Testparameter für \textit{a} bessere Laufzeiten ergeben müssten,
  da die Kommunikation auf einem Rechner und ohne Netzwerk stattfinden kann. In der Realität hat sich aber gezeigt, dass es gerade umgekehrt ist. Vermutlich behindern sich die vielen Speicherzugriffe
  bei dem Testlauf auf einem Knoten so stark, dass es effektiver ist einen Teil der Kommunikation über das Netzwerk zu führen. Auch dieser Effekt müsste aber an anderer Stelle genauer untersucht werden.
  
  